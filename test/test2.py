from urllib.parse import urljoin

import requests
from requests import Session
import re
import os
from dotenv import load_dotenv
import time
import pandas as pd
from bs4 import BeautifulSoup
from typing import List, Dict

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åŸºç¡€é…ç½®
BASE_URL = 'https://www.wenku8.net/modules/article/articlelist.php?page='
LOGIN_URL = 'https://www.wenku8.net/login.php'
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Referer': BASE_URL,
    'Origin': 'https://www.wenku8.net'
}

# ç¯å¢ƒå˜é‡é…ç½®
USERNAME = os.getenv('WENKU_USER', 'badboy44')  # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
PASSWORD = os.getenv('WENKU_PASS', '123leijuikai')
DELAY = 3
SAVE_PATH = 'books'
MIRROR_RETRY_DELAY = 3  # é•œåƒé‡è¯•é—´éš”
MIRROR_MAX_RETRIES = 2  # å•ä¸ªé•œåƒæœ€å¤§é‡è¯•æ¬¡æ•°


class LoginException(Exception):
    """è‡ªå®šä¹‰ç™»å½•å¼‚å¸¸"""
    pass


def create_authenticated_session(max_retry=3) -> Session:
    """åˆ›å»ºå¸¦å®Œæ•´è®¤è¯æµç¨‹çš„ä¼šè¯"""
    session = Session()
    session.headers.update(HEADERS)

    for attempt in range(max_retry):
        try:
            # ç¬¬ä¸€æ­¥ï¼šè·å–ç™»å½•é¡µå¹¶æå–å‚æ•°
            login_page_url = f"{LOGIN_URL}?jumpurl={requests.utils.quote(BASE_URL)}"
            login_page = session.get(login_page_url, timeout=10)
            login_page.encoding = 'gbk'

            # æå–éšè—å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
            token_match = re.search(r'name="token" value="(.*?)"', login_page.text)
            token = token_match.group(1) if token_match else ''

            # æ„é€ ç™»å½•æ•°æ®
            login_data = {
                'username': USERNAME,
                'password': PASSWORD,
                'usecookie': '2592000',
                'action': 'login',
                'submit': 'ç™»å½•',
                'token': token,
                'jumpurl': BASE_URL
            }

            # ç¬¬äºŒæ­¥ï¼šå‘é€ç™»å½•è¯·æ±‚
            response = session.post(
                f"{LOGIN_URL}?do=submit",
                data=login_data,
                allow_redirects=True,
                timeout=15
            )
            response.encoding = 'gbk'

            # ç¬¬ä¸‰æ­¥ï¼šéªŒè¯ç™»å½•çŠ¶æ€
            if 'jieqiUserInfo' not in session.cookies.get_dict():
                error_msg = extract_error_message(response.text)
                raise LoginException(f"ç™»å½•å¤±è´¥: {error_msg}")

            print("ç™»å½•æˆåŠŸï¼")
            return session

        except requests.exceptions.RequestException as e:
            if attempt == max_retry - 1:
                raise LoginException(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

    raise LoginException("è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")


def extract_error_message(html: str) -> str:
    """ä»å“åº”HTMLä¸­æå–é”™è¯¯ä¿¡æ¯"""
    error_match = re.search(r'<div class="notice">(.*?)</div>', html, re.DOTALL)
    if error_match:
        return re.sub(r'\s+', ' ', error_match.group(1)).strip()
    return "æœªçŸ¥é”™è¯¯"


def get_html(url: str, session: Session = None) -> str:
    """è·å–ç½‘é¡µå†…å®¹"""
    try:
        # å¤ç”¨ä¼šè¯æˆ–åˆ›å»ºæ–°ä¼šè¯
        if not session or not isinstance(session, Session):
            session = create_authenticated_session()

        print(f"æ­£åœ¨è®¿é—® {url}")
        response = session.get(url, timeout=15)
        response.encoding = 'gbk'

        # æ£€æŸ¥æ˜¯å¦è¢«ç™»å‡º
        if 'è¯·å…ˆç™»å½•' in response.text:
            raise LoginException("ä¼šè¯å·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°ç™»å½•")

        return response.text

    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {str(e)}")
        return None


def parse_novel_list(html: str) -> List[Dict]:
    """è§£æå°è¯´åˆ—è¡¨é¡µé¢"""
    soup = BeautifulSoup(html, 'html.parser')
    novels = []

    # å®šä½å°è¯´é¡¹å®¹å™¨
    items = soup.select('div[style*="width:373px;height:136px;float:left"]')

    for item in items:
        try:
            # æå–åŸºæœ¬ä¿¡æ¯
            title_tag = item.select_one('b > a[title]')
            detail_div = item.select_one('div:not([style*="width:95px"])')

            # åŸºç¡€ä¿¡æ¯
            novel = {
                'æ ‡é¢˜': title_tag.get_text(strip=True),
                'é“¾æ¥': 'https://www.wenku8.net' + title_tag['href'],
                'å°é¢': item.select_one('img')['src'],
                'ä½œè€…': detail_div.select('p')[0].get_text().split(':')[-1].split('/')[0].strip(),
                'å‡ºç‰ˆç¤¾': detail_div.select('p')[0].get_text().split('/')[-1].split(':')[-1].strip(),
                'æœ€åæ›´æ–°': detail_div.select('p')[1].get_text().split('/')[0].split(':')[-1].strip(),
                'å­—æ•°': detail_div.select('p')[1].get_text().split('/')[1].strip(),
                'çŠ¶æ€': detail_div.select('p')[1].get_text().split('/')[-1].strip(),
                'æ ‡ç­¾': detail_div.select('p')[2].get_text(strip=True).replace('Tags:', ''),
                'ç®€ä»‹': detail_div.select('p')[3].get_text(strip=True)
            }

            # å¤„ç†ç‰¹æ®ŠçŠ¶æ€æ ‡è®°
            if 'hottext' in detail_div.select('p')[1].get_text():
                novel['çŠ¶æ€'] = 'å·²å®Œç»“'

            novels.append(novel)
        except Exception as e:
            print(f"è§£æå°è¯´æ¡ç›®å¤±è´¥: {str(e)}")
            continue

    return novels


# def save_to_excel(data: List[Dict], filename: str):
#     """ä¿å­˜æ•°æ®åˆ°Excel"""
#     df = pd.DataFrame(data)
#
#     # åˆ—é¡ºåºè°ƒæ•´
#     columns = ['æ ‡é¢˜', 'ä½œè€…', 'å‡ºç‰ˆç¤¾', 'å­—æ•°', 'çŠ¶æ€', 'æœ€åæ›´æ–°',
#                'æ ‡ç­¾', 'ç®€ä»‹', 'é“¾æ¥', 'å°é¢']
#
#     # å»é‡å¤„ç†
#     df = df.drop_duplicates(subset=['æ ‡é¢˜', 'ä½œè€…'])
#
#     with pd.ExcelWriter(filename, engine='openpyxl') as writer:
#         df.to_excel(writer, index=False, columns=columns)
#
#     print(f"å·²ä¿å­˜ {len(df)} æ¡æ•°æ®åˆ° {filename}")
def save_to_excel(data: List[Dict], filename: str):
    """ä¿å­˜æ•°æ®åˆ°Excel"""
    try:
        df = pd.DataFrame(data)
        columns = ['æ ‡é¢˜', 'ä½œè€…', 'å‡ºç‰ˆç¤¾', 'å­—æ•°', 'çŠ¶æ€', 'æœ€åæ›´æ–°', 'æ ‡ç­¾', 'ç®€ä»‹', 'é“¾æ¥', 'å°é¢']
        df = df.drop_duplicates(subset=['æ ‡é¢˜', 'ä½œè€…'])

        # ä½¿ç”¨ç»å¯¹è·¯å¾„
        save_path = os.path.abspath(filename)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«é”å®š
        if os.path.exists(save_path):
            try:
                os.rename(save_path, save_path)  # æµ‹è¯•æ–‡ä»¶æ˜¯å¦å¯æ“ä½œ
            except OSError as e:
                print(f"é”™è¯¯ï¼šæ–‡ä»¶ {save_path} è¢«å…¶ä»–ç¨‹åºå ç”¨ï¼Œè¯·å…³é—­Excelåé‡è¯•")
                return

        # ä¿®æ”¹åçš„å†™å…¥ä»£ç 
        with pd.ExcelWriter(
                save_path,
                engine='openpyxl',
                mode='w'
        ) as writer:
            df.to_excel(writer, index=False, columns=columns)

        print(f"æˆåŠŸä¿å­˜ {len(df)} æ¡æ•°æ®åˆ° {save_path}")

    except PermissionError:
        print(f"æƒé™æ‹’ç»ï¼šè¯·æ£€æŸ¥ï¼š1. æ˜¯å¦å·²å…³é—­Excelæ–‡ä»¶ 2. æ˜¯å¦æœ‰å†™æƒé™ 3. å°è¯•ç®¡ç†å‘˜æ¨¡å¼è¿è¡Œ")
    except Exception as e:
        print(f"ä¿å­˜å¤±è´¥ï¼š{str(e)}")


def crawl_all_pages(start_page: int = 1, end_page: int = 5):
    """åˆ†é¡µæŠ“å–å°è¯´æ•°æ®"""
    all_novels = []
    session = create_authenticated_session()

    for page in range(start_page, end_page + 1):
        try:
            url = f"{BASE_URL}{page}"
            html = get_html(url, session)

            if html:
                novels = parse_novel_list(html)
                all_novels.extend(novels)
                print(f"ç¬¬ {page} é¡µæŠ“å–å®Œæˆï¼Œç´¯è®¡ {len(all_novels)} æ¡æ•°æ®")

                # éµå®ˆçˆ¬å–é—´éš”
                time.sleep(DELAY)

        except Exception as e:
            print(f"ç¬¬ {page} é¡µæŠ“å–å¤±è´¥: {str(e)}")
            continue

    return all_novels


# def parse_download_url(html: str) -> str:
#     """è§£æå°è¯´è¯¦æƒ…é¡µè·å–TXTä¸‹è½½é¡µé¢é“¾æ¥"""
#     soup = BeautifulSoup(html, 'html.parser')
#
#
# def download_txt(novel: dict, session: Session, base_url: str = "https://www.wenku8.net"):
#     """åœ¨ä¸‹è½½é¡µé¢é€šè¿‡ä¸‹è½½é“¾æ¥å¼€å§‹ä¸‹è½½"""
def parse_download_url(html: str) -> str:
    """ä»å°è¯´è¯¦æƒ…é¡µè§£æTXTå…¨æœ¬ä¸‹è½½é¡µé¢é“¾æ¥"""
    soup = BeautifulSoup(html, 'html.parser')

    # å®šä½TXTå…¨æœ¬ä¸‹è½½æŒ‰é’®
    download_link = soup.find('a', string='TXTç®€ç¹å…¨æœ¬')
    if not download_link:
        download_link = soup.find('a', href=lambda x: x and 'type=txtfull' in x)

    if download_link:
        return urljoin("https://www.wenku8.net", download_link['href'])
    raise ValueError("æœªæ‰¾åˆ°å…¨æœ¬ä¸‹è½½é“¾æ¥")


# def download_txt(novel: dict, session: Session, base_url: str = "https://www.wenku8.net"):
#     """æ‰§è¡Œå®Œæ•´ä¸‹è½½æµç¨‹"""
#     try:
#         # ç¬¬ä¸€æ­¥ï¼šè·å–ä¸‹è½½é¡µé¢
#         download_page_url = parse_download_url(novel['html_content'])
#         print(f"æ­£åœ¨è·å–ä¸‹è½½é¡µé¢ï¼š{download_page_url}")
#
#         # ç¬¬äºŒæ­¥ï¼šè®¿é—®ä¸‹è½½é¡µé¢
#         download_page = session.get(download_page_url)
#         download_page.encoding = 'gbk'
#
#         # ç¬¬ä¸‰æ­¥ï¼šè§£æä¸‹è½½é“¾æ¥
#         soup = BeautifulSoup(download_page.text, 'html.parser')
#         download_table = soup.find('table', {'class': 'grid'})
#
#         # å®šä½ç®€ä½“ä¸‹è½½é“¾æ¥
#         simplified_links = []
#         for row in download_table.find_all('tr'):
#             if 'ç®€ä½“(G)' in row.text:
#                 links = row.find_all('a', href=True)
#                 simplified_links = [urljoin(base_url, l['href']) for l in links if 'type=txt' in l['href']]
#                 break
#
#         if not simplified_links:
#             raise ValueError("æœªæ‰¾åˆ°ç®€ä½“ä¸‹è½½é“¾æ¥")
#
#         # ç¬¬å››æ­¥ï¼šä¸‹è½½æ–‡ä»¶
#         for i, dl_url in enumerate(simplified_links[:2], 1):  # å–å‰2ä¸ªé•œåƒ
#             print(f"æ­£åœ¨ä¸‹è½½ç¬¬{i}ä¸ªé•œåƒï¼š{dl_url}")
#             response = session.get(dl_url, stream=True)
#
#             # ä»URLæå–æ–‡ä»¶å
#             filename = f"{novel['æ ‡é¢˜']}_ç®€ä½“ç‰ˆ_{i}.txt"
#             invalid_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
#             for char in invalid_chars:
#                 filename = filename.replace(char, '_')
#
#             save_path = os.path.join(SAVE_PATH, filename)
#
#             with open(save_path, 'wb') as f:
#                 for chunk in response.iter_content(chunk_size=8192):
#                     if chunk:
#                         f.write(chunk)
#             print(f"å·²ä¿å­˜åˆ°ï¼š{save_path}")
#             time.sleep(DELAY)
#
#     except Exception as e:
#         print(f"ä¸‹è½½å¤±è´¥ï¼š{str(e)}")

def download_txt(novel: dict, session: Session, base_url: str = "https://www.wenku8.net"):
    """æ™ºèƒ½ä¸‹è½½æµç¨‹ï¼ˆå¸¦é•œåƒé‡è¯•æœºåˆ¶ï¼‰"""
    try:
        # è·å–ä¸‹è½½é¡µé¢
        download_page_url = parse_download_url(novel['html_content'])
        print(f"æ­£åœ¨è§£æä¸‹è½½é¡µé¢ï¼š{download_page_url}")

        # è®¿é—®ä¸‹è½½é¡µ
        download_page = session.get(download_page_url)
        download_page.encoding = 'gbk'

        # è§£æä¸‹è½½é€‰é¡¹
        soup = BeautifulSoup(download_page.text, 'html.parser')
        download_table = soup.find('table', {'class': 'grid'})

        # æå–æ‰€æœ‰ç®€ä½“é•œåƒé“¾æ¥
        simplified_links = []
        for row in download_table.find_all('tr'):
            if 'ç®€ä½“' in row.text:
                links = [urljoin(base_url, l['href'])
                         for l in row.find_all('a', href=True)
                         if 'type=txt' in l['href']]
                simplified_links.extend(links)
                break

        if not simplified_links:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆä¸‹è½½é“¾æ¥")

        # æ™ºèƒ½ä¸‹è½½é€»è¾‘
        success = False
        for idx, dl_url in enumerate(simplified_links, 1):
            retry_count = 0
            max_retries = 2  # æœ€å¤§é‡è¯•æ¬¡æ•°

            while retry_count <= max_retries and not success:
                try:
                    print(f"å°è¯•é•œåƒ{idx}{f' ç¬¬{retry_count + 1}æ¬¡é‡è¯•' if retry_count > 0 else ''}: {dl_url}")
                    response = session.get(dl_url, stream=True, timeout=20)
                    response.raise_for_status()

                    # æ„å»ºå®‰å…¨æ–‡ä»¶å
                    filename = f"{novel['æ ‡é¢˜']}_ç®€ä½“ç‰ˆ.txt"
                    invalid_chars = {'/', '\\', ':', '*', '?', '"', '<', '>', '|'}
                    filename = ''.join([c if c not in invalid_chars else '_' for c in filename])
                    save_path = os.path.join(SAVE_PATH, filename)

                    # æµå¼å†™å…¥æ–‡ä»¶
                    with open(save_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)

                    print(f"æˆåŠŸä¿å­˜åˆ°ï¼š{save_path}")
                    success = True
                    break

                except requests.exceptions.RequestException as e:
                    if retry_count < max_retries:
                        print(f"é•œåƒ{idx}ä¸‹è½½å¤±è´¥ï¼Œ3ç§’åé‡è¯•... é”™è¯¯ï¼š{str(e)}")
                        time.sleep(3)
                        retry_count += 1
                    else:
                        print(f"é•œåƒ{idx}è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªé•œåƒ")
                        break  # è·³å‡ºé‡è¯•å¾ªç¯ï¼Œåˆ‡æ¢é•œåƒ

                except IOError as e:
                    print(f"æ–‡ä»¶å†™å…¥å¤±è´¥ï¼š{str(e)}")
                    break  # æ–‡ä»¶ç³»ç»Ÿé”™è¯¯ç›´æ¥ç»ˆæ­¢

            if success:
                break  # æˆåŠŸåˆ™ç»ˆæ­¢é•œåƒå¾ªç¯

        if not success:
            raise Exception("æ‰€æœ‰é•œåƒå‡ä¸å¯ç”¨")

    except Exception as e:
        print(f"ä¸‹è½½æµç¨‹å¤±è´¥ï¼š{str(e)}")
        raise


def download_all(novels: list[dict], total_count: int):
    # åˆå§‹åŒ–ç»Ÿè®¡æŒ‡æ ‡
    start_time = time.time()
    total_count = total_count
    success_count = 0
    fail_count = 0
    total_size = 0  # å•ä½ï¼šå­—èŠ‚
    # ä¸‹è½½å¤„ç†å¾ªç¯

    for idx, novel in enumerate(novels, 1):
        novel_title = novel['æ ‡é¢˜']
        print(f"\n[{idx}/{total_count}] æ­£åœ¨å¤„ç†ï¼š{novel_title}")

        try:
            # è·å–è¯¦æƒ…é¡µ
            novel['html_content'] = get_html(novel['é“¾æ¥'], auth_session)

            # æ‰§è¡Œä¸‹è½½
            start_dl = time.time()
            download_txt(novel, auth_session)

            # è®¡ç®—æ–‡ä»¶å¤§å°
            filename = f"{novel_title}_ç®€ä½“ç‰ˆ.txt".translate(str.maketrans('', '', r'\/:*?"<>|'))
            file_path = os.path.join(SAVE_PATH, filename)
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                total_size += file_size
                print(f"æ–‡ä»¶å¤§å°ï¼š{file_size / 1024:.2f} KB")

            success_count += 1
            time.sleep(2)

        except Exception as e:
            fail_count += 1
            print(f"â—{novel_title} å¤„ç†å¤±è´¥ï¼š{str(e)}")
            continue
            # ç»Ÿè®¡ç»“æœè¾“å‡º
    end_time = time.time()
    time_cost = end_time - start_time
    print("\n" + "=" * 40)
    print("ğŸ“Š ä»»åŠ¡ç»Ÿè®¡æŠ¥å‘Š")
    print("-" * 40)
    print(f"æ€»å¤„ç†æ•°é‡ï¼š{total_count}")
    print(f"âœ… æˆåŠŸæ•°é‡ï¼š{success_count} ({success_count / total_count:.1%})")
    print(f"âŒ å¤±è´¥æ•°é‡ï¼š{fail_count} ({fail_count / total_count:.1%})")
    print(f"â±ï¸ æ€»è€—æ—¶ï¼š{time_cost // 3600:.0f}å°æ—¶{time_cost % 3600 // 60:.0f}åˆ†{time_cost % 60:.2f}ç§’")
    print(f"ğŸ“¦ æ€»ä¸‹è½½é‡ï¼š{total_size / 1024 / 1024:.2f} MB")
    print("=" * 40)


if __name__ == '__main__':

    # åˆå§‹åŒ–å…¨å±€ä¼šè¯
    auth_session = create_authenticated_session()
    os.makedirs(SAVE_PATH, exist_ok=True)

    # æŠ“å–å‰2é¡µæ•°æ®
    novels_data = crawl_all_pages(start_page=1, end_page=2)
    total_count = len(novels_data)

    # ä¿å­˜ç»“æœ
    if novels_data:
        save_to_excel(novels_data, 'novels_list.xlsx')

    download_all(novels_data, total_count)

# if __name__ == '__main__':
#     # åˆå§‹åŒ–å…¨å±€ä¼šè¯
#     auth_session = create_authenticated_session()
#
#     # æµ‹è¯•è®¿é—®
#     # test_url = f"{BASE_URL}1"
#     # html_content = get_html(test_url, auth_session)
#
#     # æŠ“å–å‰5é¡µæ•°æ®
#     novels_data = crawl_all_pages(start_page=1, end_page=2)
#
#     #ä»novels_dataä¸­è®¿é—®æ¯ä¸ªå°è¯´çš„é“¾æ¥å¹¶ä»ä¸­ä¸‹è½½TXT
#     for novel in novels_data:
#         novel_url = novel['é“¾æ¥']
#         html_content = get_html(novel_url, auth_session)
#         parse_download_url(html_content)
#
#
#     # ä¿å­˜ç»“æœ
#     if novels_data:
#         save_to_excel(novels_data, 'novels_list.xlsx')
# print(html_content)
# if html_content:
#     with open('output.html', 'w', encoding='gbk') as f:
#         f.write(html_content)
#     print("é¡µé¢å·²ä¿å­˜è‡³output.html")
